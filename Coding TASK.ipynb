{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkwGp9W68BcWibeVVJ0WL3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshitaBadiyasar/SuperAGI-Assignment/blob/main/Coding%20TASK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 1"
      ],
      "metadata": {
        "id": "TS-clCXMjJys"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Configuration for a small GPT-2 model\n",
        "class GPT2Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.max_position_embeddings = 1024\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 12\n",
        "        self.n_embd = 768\n",
        "        self.layer_norm_epsilon = 1e-5\n",
        "        self.initializer_range = 0.02\n",
        "\n",
        "# Scaled dot-product attention function\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "    temp = query.bmm(key.transpose(1, 2)) / math.sqrt(query.size(-1))\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    return softmax(temp).bmm(value)\n",
        "\n",
        "# Single attention head\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embd_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embd_dim, embd_dim)\n",
        "        self.key = nn.Linear(embd_dim, embd_dim)\n",
        "        self.value = nn.Linear(embd_dim, embd_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        return scaled_dot_product_attention(\n",
        "            self.query(hidden_state), self.key(hidden_state), self.value(hidden_state)\n",
        "        )\n",
        "\n",
        "# Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embd_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([AttentionHead(embd_dim) for _ in range(n_heads)])\n",
        "        self.linear = nn.Linear(n_heads * embd_dim, embd_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attention = [head(hidden_state) for head in self.heads]\n",
        "        concatenated = torch.cat(attention, dim=-1)\n",
        "        return self.linear(concatenated)\n",
        "\n",
        "# Pointwise Feed Forward layer\n",
        "class PointwiseFeedForward(nn.Module):\n",
        "    def __init__(self, embd_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embd_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embd_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        return self.linear2(nn.functional.relu(self.linear1(hidden_state)))\n",
        "\n",
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embd_dim, n_heads, ff_dim, layer_norm_epsilon):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(embd_dim, n_heads)\n",
        "        self.feed_forward = PointwiseFeedForward(embd_dim, ff_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(embd_dim, eps=layer_norm_epsilon)\n",
        "        self.layer_norm2 = nn.LayerNorm(embd_dim, eps=layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attention_output = self.attention(hidden_state)\n",
        "        norm1 = self.layer_norm1(hidden_state + attention_output)\n",
        "        feed_forward_output = self.feed_forward(norm1)\n",
        "        norm2 = self.layer_norm2(norm1 + feed_forward_output)\n",
        "        return norm2\n",
        "\n",
        "# GPT-2 model\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embd_dim = config.n_embd\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, self.embd_dim)\n",
        "        self.position_embedding = nn.Embedding(config.max_position_embeddings, self.embd_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(self.embd_dim, config.n_heads, 4 * self.embd_dim, config.layer_norm_epsilon)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, input_ids, positions_ids=None):\n",
        "        if positions_ids is None:\n",
        "            positions_ids = torch.arange(0, input_ids.size(1)).unsqueeze(0).to(input_ids.device)\n",
        "        tokens = self.token_embedding(input_ids)\n",
        "        positions = self.position_embedding(positions_ids)\n",
        "\n",
        "        x = tokens + positions\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration setup\n",
        "    config = GPT2Config()\n",
        "    # Create GPT-2 model instance\n",
        "    model = GPT2(config)\n",
        "\n",
        "    # Generate random input for demonstration\n",
        "    input_ids = torch.randint(0, config.vocab_size, (1, 1024))\n",
        "    # Obtain model output\n",
        "    output = model(input_ids)\n",
        "    print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "270xIfDjjSkb",
        "outputId": "87fea060-d962-427b-bb8f-f2d26093e68e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.9850, -1.3691,  0.2589,  ...,  1.3280, -1.0538,  0.6917],\n",
            "         [ 0.8623,  0.3825, -1.0204,  ...,  0.1670, -0.1596, -1.5403],\n",
            "         [-1.8322, -0.9128, -1.1802,  ..., -0.4246, -0.1565, -0.3061],\n",
            "         ...,\n",
            "         [-0.8388, -1.6704, -1.3986,  ..., -0.3119,  0.8182, -0.4821],\n",
            "         [ 1.7029,  0.7901, -1.1380,  ...,  0.0844,  0.6409,  1.0030],\n",
            "         [ 0.6698, -0.1128, -0.7353,  ...,  0.6422,  1.6727,  0.0911]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 2"
      ],
      "metadata": {
        "id": "6oAG1mHhjH_K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "i_EMV2zVfGUJ"
      },
      "outputs": [],
      "source": [
        "#Rotary Positional Embedding\n",
        "import torch\n",
        "\n",
        "def apply_rotary_pos_emb(x, sincos):\n",
        "    sin, cos = map(lambda t: t.repeat_interleave(2, dim=-1), sincos)\n",
        "    return (x * cos) + (torch.roll(x, shifts=1, dims=-1) * sin)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Group Query Attention\n",
        "def group_query_attention(query, key, value, num_groups):\n",
        "    # Split queries into groups\n",
        "    group_size = query.size(2) // num_groups\n",
        "    query_groups = query.view(*query.size()[:2], num_groups, group_size)\n",
        "\n",
        "    # Perform attention within each group\n",
        "    attention_output = []\n",
        "    for i in range(num_groups):\n",
        "        group_attn_output = scaled_dot_product_attention(query_groups[:,:,i,:], key, value)\n",
        "        attention_output.append(group_attn_output)\n",
        "\n",
        "    # Concatenate the outputs of each group\n",
        "    return torch.cat(attention_output, dim=-1)\n"
      ],
      "metadata": {
        "id": "rG0-ZrODi02q"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sliding Window Attention\n",
        "def sliding_window_attention(query, key, value, window_size):\n",
        "    # Assume query, key, and value are all the same size for simplicity\n",
        "    batch_size, seq_length, dim = query.size()\n",
        "    attention_scores = torch.empty((batch_size, seq_length, window_size), device=query.device)\n",
        "\n",
        "    # Compute attention scores for a sliding window\n",
        "    for i in range(seq_length):\n",
        "        start = max(0, i - window_size // 2)\n",
        "        end = min(seq_length, i + window_size // 2 + 1)\n",
        "        attention_scores[:, i, :end-start] = torch.bmm(query[:, i:i+1, :], key[:, start:end, :].transpose(1, 2))\n",
        "\n",
        "    # Apply softmax to get attention probabilities\n",
        "    attention_probs = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    # Compute weighted sum to get the attention output\n",
        "    attention_output = torch.bmm(attention_probs, value[:, start:end, :])\n",
        "    return attention_output\n"
      ],
      "metadata": {
        "id": "KiB0Ap0Pi4sX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TASK 3"
      ],
      "metadata": {
        "id": "FjPxYpAdi7lu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distributed Data Parallel"
      ],
      "metadata": {
        "id": "W_vzqlOPjcQp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fully Sharded Data Parallel"
      ],
      "metadata": {
        "id": "OOnu4sfDjewv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2OIaXRbqjhPh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}