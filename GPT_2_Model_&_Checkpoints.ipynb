{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJRreBvUpv/cwAAvgVkvSW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshitaBadiyasar/SuperAGI-Assignment/blob/main/GPT_2_Model_%26_Checkpoints.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh9hLAXYW_Pb",
        "outputId": "0d7c0e27-e474-4fa3-83a6-ab592e4e4163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.5960, -1.6826, -0.8329,  ...,  0.6209, -0.2323,  1.3404],\n",
            "         [-1.4362, -0.8451,  0.8915,  ..., -0.5067, -1.3798,  0.0387],\n",
            "         [ 0.8700, -1.5147, -0.7995,  ...,  0.5665,  0.2666, -0.5611],\n",
            "         ...,\n",
            "         [ 0.4757, -0.7607,  0.2302,  ..., -0.2831,  0.4556,  0.9543],\n",
            "         [ 2.2712, -0.3046, -0.1703,  ...,  0.1419,  0.6453, -0.8552],\n",
            "         [-0.2020, -1.7010, -1.1221,  ...,  0.0676, -0.0682,  0.6557]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Configuration for a small GPT-2 model\n",
        "class GPT2Config:\n",
        "    def __init__(self):\n",
        "        self.vocab_size = 50257\n",
        "        self.max_position_embeddings = 1024\n",
        "        self.n_layers = 12\n",
        "        self.n_heads = 12\n",
        "        self.n_embd = 768\n",
        "        self.layer_norm_epsilon = 1e-5\n",
        "        self.initializer_range = 0.02\n",
        "\n",
        "# Scaled dot-product attention function\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "    temp = query.bmm(key.transpose(1, 2)) / math.sqrt(query.size(-1))\n",
        "    softmax = nn.Softmax(dim=-1)\n",
        "    return softmax(temp).bmm(value)\n",
        "\n",
        "# Single attention head\n",
        "class AttentionHead(nn.Module):\n",
        "    def __init__(self, embd_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(embd_dim, embd_dim)\n",
        "        self.key = nn.Linear(embd_dim, embd_dim)\n",
        "        self.value = nn.Linear(embd_dim, embd_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        return scaled_dot_product_attention(\n",
        "            self.query(hidden_state), self.key(hidden_state), self.value(hidden_state)\n",
        "        )\n",
        "\n",
        "# Multi-head attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embd_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([AttentionHead(embd_dim) for _ in range(n_heads)])\n",
        "        self.linear = nn.Linear(n_heads * embd_dim, embd_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attention = [head(hidden_state) for head in self.heads]\n",
        "        concatenated = torch.cat(attention, dim=-1)\n",
        "        return self.linear(concatenated)\n",
        "\n",
        "# Pointwise Feed Forward layer\n",
        "class PointwiseFeedForward(nn.Module):\n",
        "    def __init__(self, embd_dim, ff_dim):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(embd_dim, ff_dim)\n",
        "        self.linear2 = nn.Linear(ff_dim, embd_dim)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        return self.linear2(nn.functional.relu(self.linear1(hidden_state)))\n",
        "\n",
        "# Transformer block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embd_dim, n_heads, ff_dim, layer_norm_epsilon):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(embd_dim, n_heads)\n",
        "        self.feed_forward = PointwiseFeedForward(embd_dim, ff_dim)\n",
        "        self.layer_norm1 = nn.LayerNorm(embd_dim, eps=layer_norm_epsilon)\n",
        "        self.layer_norm2 = nn.LayerNorm(embd_dim, eps=layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        attention_output = self.attention(hidden_state)\n",
        "        norm1 = self.layer_norm1(hidden_state + attention_output)\n",
        "        feed_forward_output = self.feed_forward(norm1)\n",
        "        norm2 = self.layer_norm2(norm1 + feed_forward_output)\n",
        "        return norm2\n",
        "\n",
        "# GPT-2 model\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embd_dim = config.n_embd\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, self.embd_dim)\n",
        "        self.position_embedding = nn.Embedding(config.max_position_embeddings, self.embd_dim)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(self.embd_dim, config.n_heads, 4 * self.embd_dim, config.layer_norm_epsilon)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "        self.layer_norm = nn.LayerNorm(self.embd_dim, eps=config.layer_norm_epsilon)\n",
        "\n",
        "    def forward(self, input_ids, positions_ids=None):\n",
        "        if positions_ids is None:\n",
        "            positions_ids = torch.arange(0, input_ids.size(1)).unsqueeze(0).to(input_ids.device)\n",
        "        tokens = self.token_embedding(input_ids)\n",
        "        positions = self.position_embedding(positions_ids)\n",
        "\n",
        "        x = tokens + positions\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.layer_norm(x)\n",
        "        return x\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration setup\n",
        "    config = GPT2Config()\n",
        "    # Create GPT-2 model instance\n",
        "    model = GPT2(config)\n",
        "\n",
        "    # Generate random input for demonstration\n",
        "    input_ids = torch.randint(0, config.vocab_size, (1, 1024))\n",
        "    # Obtain model output\n",
        "    output = model(input_ids)\n",
        "    print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xUMw6QtLZmhh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}